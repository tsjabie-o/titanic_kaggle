{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction (Kaggle)\n",
    "This is my first project on Kaggle. We have been given a dataset of passengers onboard the Titanic, along with whether they survived the crash or not. Our job is to predict this survival variable for the passengers in the test set.\n",
    "I'm going to keep this relatively simple, but I might revisit and improve my models at some point. For now, I'll go with some basic data cleaning and feature- and model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the libraries we'll need\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training set into memory\n",
    "dat_train = pd.read_csv(\"train.csv\")\n",
    "dat_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data and selecting the features\n",
    "I will remove the `Name`, `Ticket`, `SibSp`, `Parch` and `Cabin` features because intuitively they seem to have no influence over the chance of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_train = dat_train.drop([\"Name\", \"Ticket\", \"SibSp\", \"Parch\", \"Cabin\"], axis = 1)\n",
    "dat_test = dat_test.drop([\"Name\", \"Ticket\", \"SibSp\", \"Parch\", \"Cabin\"], axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for any missing values and fill them in as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Sex              0\n",
      "Age            177\n",
      "Fare             0\n",
      "Embarked         2\n",
      "dtype: int64\n",
      " \n",
      "PassengerId     0\n",
      "Pclass          0\n",
      "Sex             0\n",
      "Age            86\n",
      "Fare            1\n",
      "Embarked        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dat_train.isnull().sum())\n",
    "print(\" \")\n",
    "print(dat_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId    0\n",
      "Survived       0\n",
      "Pclass         0\n",
      "Sex            0\n",
      "Age            0\n",
      "Fare           0\n",
      "Embarked       0\n",
      "dtype: int64\n",
      " \n",
      "PassengerId    0\n",
      "Pclass         0\n",
      "Sex            0\n",
      "Age            0\n",
      "Fare           0\n",
      "Embarked       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filling the missing embarked and fare data with the most frequent value\n",
    "dat_train[\"Embarked\"].fillna(dat_train[\"Embarked\"].mode()[0], inplace=True)\n",
    "dat_test[\"Fare\"].fillna(dat_test[\"Fare\"].mode()[0], inplace=True)\n",
    "\n",
    "# Filling the missing age values using the median\n",
    "for dat in [dat_train, dat_test]:\n",
    "    dat[\"Age\"].fillna(dat[\"Age\"].median(), inplace=True)\n",
    "\n",
    "print(dat_train.isnull().sum())\n",
    "print(\" \")\n",
    "print(dat_test.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also be converting the categorical values into numerical ones. To prevent our model from intepreting some kind of order to the nominal values (such as male = 0, female = 1) we will use one-hot encoding. We will use this encoding for the `Sex` and `Embarked` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit(dat_train[[\"Sex\", \"Embarked\"]])\n",
    "transformed_train = pd.DataFrame(enc.transform(dat_train[[\"Sex\", \"Embarked\"]]).toarray(), columns=enc.get_feature_names_out())\n",
    "transformed_test = pd.DataFrame(enc.transform(dat_test[[\"Sex\", \"Embarked\"]]).toarray(), columns=enc.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_train = dat_train.drop(labels=[\"Sex\", \"Embarked\"], axis = 1)\n",
    "dat_test = dat_test.drop(labels=[\"Sex\", \"Embarked\"], axis=1)\n",
    "\n",
    "dat_train = pd.concat([dat_train, transformed_train], axis=1)\n",
    "dat_test = pd.concat([dat_test, transformed_test], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also split up the training data into X and y, and split those into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dat_train.drop(\"Survived\", axis=1), dat_train[\"Survived\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=33)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "Let us try logistic regression on this problem and see how it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8033898305084746"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_model = LogisticRegression(max_iter=300)\n",
    "lm_model.fit(X_train, y_train)\n",
    "lm_model.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "We will aso try a random forest classifier on this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8305084745762712"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(random_state=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_model.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an accuracy of about 0.8 for the logistic regression model and about 0.83 for the random forest model. I will use the random forest model to do predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(dat_test)\n",
    "submission = pd.concat([dat_test[\"PassengerId\"], pd.DataFrame(y_pred, columns=[\"Survived\"])], axis=1).set_index(\"PassengerId\")\n",
    "submission.to_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
